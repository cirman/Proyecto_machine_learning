{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (Analisis Exploratorio de Datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will import the libraries used for machine learning\n",
    "\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # this is used for the plot the graph \n",
    "import seaborn as sns # used for plot interactive graph. I like it most for plot\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression # to apply the Logistic regression\n",
    "from sklearn.model_selection import train_test_split # to split the data into two parts\n",
    "from sklearn.model_selection import KFold # use for cross validation\n",
    "from sklearn.model_selection import GridSearchCV# for tuning parameter\n",
    "from sklearn.ensemble import RandomForestClassifier # for random forest classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm # for Support Vector Machine\n",
    "from sklearn import metrics # for the check the error and accuracy of the model\n",
    "from plotnine import * # incluye funciones de ggplot\n",
    "from sklearn.svm import SVC # support vector machine\n",
    "from sklearn.model_selection import cross_val_score # validacion cruzada\n",
    "from sklearn.model_selection import GridSearchCV # grid\n",
    "# Any results you write to the current directory are saved as output.\n",
    "# dont worry about the error if its not working then insteda of model_selection we can use cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\".\\data.csv\",header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reemplazamos las etiquetas desde strings a numeros: 1 = Maligno; 0 = Beningno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.diagnosis = data.diagnosis.replace({\"M\":1, \"B\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación revisamos la estructura del dataframe y de las variables que contiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contamos los valores perdidos para cada variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso el interés es clasificar correctamente los diagnosticos de una base datos. A continuación visualizamos los diagnosticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (data\n",
    "                       .groupby(\"diagnosis\")\n",
    "                       .agg(frequency=(\"diagnosis\", \"count\"))\n",
    "                       .reset_index())\n",
    "\n",
    "(ggplot(a) +\n",
    "  geom_bar(aes(x = \"diagnosis\", y = \"frequency\"), stat = 'identity'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que la mayoría de diagnosticos son de tumores benignos y los malignos se presentan en una menor proporción. A continuación examinamos la distribución de los diagnosticos de acuerdo con otras variables de la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(data.corr(),annot=True,fmt='.0%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de la matriz de correlacion se observa que hay grupos de variables que se relacionan entre sí. Por ejemplo, las variables radio, perimetro y area tienen una fuerte correlacion entre ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = sns.FacetGrid(data, hue=\"diagnosis\",aspect=4)\n",
    "facet.map(sns.kdeplot,'radius_mean',shade= True)\n",
    "facet.set(xlim=(0, data['radius_mean'].max()))\n",
    "facet.add_legend() \n",
    "plt.show()\n",
    "\n",
    "facet = sns.FacetGrid(data, hue=\"diagnosis\",aspect=4)\n",
    "facet.map(sns.kdeplot,'texture_mean',shade= True)\n",
    "facet.set(xlim=(0, data['texture_mean'].max()))\n",
    "facet.add_legend() \n",
    "plt.xlim(10,40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los tumores malignos, presentan un radio promedio mayor en comparación con los tumores malignos. Mientras que con respecto a la textura (desviación estándar de los valores de la escala de grises), los tumores malignos también muestran una puntuación promedio más alta, que los tumores benignos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"diagnosis\", \"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\"]\n",
    "\n",
    "sns.pairplot(data[cols], hue=\"diagnosis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acuerdo con los diagramas de densidad, se aprecia que las variables que mejor permiten diferenciar el tipo de tumor, son el perimetro, el area y el radio, ya que en la variable textura, ambos grupos exhiben un alto solapamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(data['texture_mean'])\n",
    "\n",
    "area = np.pi * (15 * np.random.rand( size ))**2\n",
    "colors = np.random.rand( size )\n",
    "\n",
    "plt.xlabel(\"texture mean\")\n",
    "plt.ylabel(\"radius mean\") \n",
    "plt.scatter(data['texture_mean'], data['radius_mean'], s=area, c=colors, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Componentes principales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación seleccionamos las variables cuantitativas para reducir la dimensionalidad del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=data.iloc[:,2:32] # excluimos la variable de indentificación y la de diagnostico\n",
    "samples.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación escalamos las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler() # el reescalador mixmax\n",
    "\n",
    "scaler.fit(samples)\n",
    "samples_scaled = scaler.transform(samples)\n",
    "print(samples_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora examinamos los valores propios para determinar el numero de componentes que debemos extraer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar paquetes\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# creamos el modelo y ajustamos\n",
    "model = PCA()\n",
    "model.fit(samples_scaled)\n",
    "\n",
    "# crear un rango que enumere las característica del ACP\n",
    "caract = range(model.n_components_)\n",
    "\n",
    "# grafiquemos la varianza explicada del modelo ACP\n",
    "plt.bar(caract,model.explained_variance_)\n",
    "plt.xticks(caract)\n",
    "plt.ylabel('Varianza')\n",
    "plt.xlabel('variables del ACP')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizando la varianza explicada por cada componente, parece suficiente extraer cuatro componentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "principalComponents = pca.fit_transform(samples_scaled)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['PC 1', 'PC 2','PC 3','PC 4'])\n",
    "principalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.sum() # varianza explicada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya obtuvimos las puntuaciones de cada observación en los cuatro componentes. Ahora procedemos a agregar estas puntuaciones en la BD original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new=pd.concat([data[['diagnosis']],principalDf], axis = 1)\n",
    "data_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de entrenar el modelo nos aseguramos que haya ambos tipos de diganosticos al momento de divir las muestras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análisis de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_new, data_new.diagnosis, random_state=0)\n",
    "print(\"Size of training set: {} size of test set: {}\".format(X_train.shape[0], X_test.shape[0]))\n",
    "best_score = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos el modelo con mejor rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for gamma in [0.01, 0.1, 1, 10]:\n",
    "#    for C in [0.01, 0.1, 1, 10]:\n",
    " #       svm = SVC(gamma=gamma, C=C) # Entrena SVC para cada parámetro\n",
    "  #      scores = cross_val_score(svm, X_train, y_train, cv=5) # Calcula validación cruzada\n",
    "   #     score = np.mean(scores) # Calcula media de la validación cruzada para precisión\n",
    "    ##    if score > best_score:\n",
    "      #      best_score = score\n",
    "       #     best_parameters = {'C': C, 'gamma': gamma}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm = SVC(**best_parameters)\n",
    "#svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya sabemos que el modelo con C= 1 y gamma=0.01 es el que tiene mejor rendimiento. Ahora procedemos a validarlo con el metodo gridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter grid:\n",
      "{'C': [0.01, 0.1, 1, 10], 'gamma': [0.01, 0.1, 1, 10]}\n",
      "Test set score: 1.00\n",
      "Best parameters: {'C': 0.1, 'gamma': 0.1}\n",
      "Best cross-validation score: 1.00\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.01, 0.1, 1, 10],\n",
    "              'gamma': [0.01, 0.1, 1, 10]}\n",
    "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))\n",
    "\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "df95319d8ce4e1d89f5365ae10992bc1f65da593082b1d264e8f529830ec2f02"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
